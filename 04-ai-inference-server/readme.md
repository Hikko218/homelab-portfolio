# AI Inference Server

GPU-powered Linux server for running Large Language Models (LLMs) inside the HomeLab environment.

Provides secure, private, and cost-efficient inference without depending on external cloud providers.

---

## Table of Contents
- [Purpose & Overview](#purpose--overview)
- [Hardware & GPU](#hardware--gpu)
- [Dashboard](#dashboard)
- [LLM Runtime & Tools](#llm-runtime--tools)
- [Supported Models](#supported-models)
- [Performance Considerations](#performance-considerations)
- [Skills Demonstrated](#skills-demonstrated)
- [Related Projects](#related-projects)

---

## Purpose & Overview

This project delivers a GPU-powered platform for running AI workloads in the HomeLab.  
It is focused on:

- **Local inference & privacy-first AI**
- Understanding hardware limitations
- Experimenting with different LLM families
- Supporting infrastructure automation in the future

> No cloud dependency ‚Äî full data control.

---

## Hardware & GPU

| Component | Details |
|----------|---------|
| Host | Ubuntu Server |
| CPU | Intel i5-11600KF |
| RAM | 32 GB DDR4 |
| GPU | **NVIDIA GTX 1060 (3 GB VRAM)** |
| Storage | 500 GB SSD |

This configuration is optimized for **compact LLMs** with quantization.

---

## Dashboard

<p align="center">
<img src="./screenshots/open_web_ui.png" width="1250">
</p>

- Inference server inside **Management VLAN** for restricted access
- **Docker-based** deployment of *Open WebUI*
- **Ollama backend** for model execution and caching
- Local browser clients inside HomeLab network

Flow example:

Client ‚Üí Open WebUI Container ‚Üí Ollama Runtime ‚Üí GPU (CUDA) ‚Üí LLM Response

---

## LLM Runtime & Tools

| Component | Purpose |
|----------|---------|
| **Open WebUI** | Web-based chat interface |
| **Ollama** | Local model runtime & download manager |
| **CUDA** | GPU acceleration |
| **Docker** | Reliable app lifecycle management |

No Internet required once models are downloaded.

---

## Supported Models

LLMs tested within VRAM limitations of the GTX 1060 (3 GB):

| Model Family | Size | GPU Working? | Notes |
|--------------|-----:|:------------:|------|
| **Phi-3 Mini** | ~3.8B | üü¢ | Best performance |
| Mistral | 7B | üü° | Works with heavy quantization |
| Llama | 7B | üü° | Slow due to VRAM limits |
| Qwen2.5 | 7B | üü° | Reduced context recommended |

> Recommended for this GPU: Phi-3 Mini (fastest + most stable)

---

## Performance Considerations

- GPU memory is the key bottleneck
- Quantized models required for 3 GB VRAM
- Best runtime results with:
  - Shorter context windows
  - Simpler models (3‚Äì5B parameters)
- Local inference still responds **within seconds** on small models

> This system balances cost vs. capability for real AI experimentation ‚öñÔ∏è

---

## Skills Demonstrated

- Linux-based GPU server administration
- Docker container deployment
- GPU drivers & CUDA support configuration
- Local AI/ML inference architecture
- Model testing and performance evaluation
- Private AI operations without cloud reliance

---

## Related Projects

- [01-homelab-infrastructure](https://github.com/Hikko218/homelab-portfolio/tree/main/01-homelab-infrastructure)
- [02-enterprise-infrastructure](https://github.com/Hikko218/homelab-portfolio/tree/main/02-enterprise-infrastructure) 
- [03-monitoring](https://github.com/Hikko218/homelab-portfolio/tree/main/03-monitoring) 
- [05-nextcloud-server](https://github.com/Hikko218/homelab-portfolio/tree/main/05-nextcloud-server) 

---

üìå *This server enables practical LLM testing and prepares for future automation in the HomeLab.*
